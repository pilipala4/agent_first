import re
import os
import json
import logging
from typing import Dict, Any, Optional, List, Union
from openai import OpenAI
from dotenv import load_dotenv
from functools import wraps

from openai._exceptions import (
    AuthenticationError,
    RateLimitError,
    APIConnectionError,
    APIError
)

# å°è¯•å¯¼å…¥è¶…æ—¶å¼‚å¸¸ï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬
try:
    from openai._exceptions import APITimeoutError as Timeout
except ImportError:
    try:
        from openai import APITimeoutError as Timeout
    except ImportError:
        class Timeout(Exception):
            pass

load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_calls.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é…ç½®å¸¸é‡
DEFAULT_MODEL = "qwen3-32b"
DEFAULT_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"


def handle_llm_exceptions(func):
    """è£…é¥°å™¨ï¼šç»Ÿä¸€å¤„ç† LLM API å¼‚å¸¸"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except AuthenticationError as e:
            error_msg = f"è®¤è¯é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return _create_error_result("AuthenticationError", error_msg)
        except RateLimitError as e:
            error_msg = f"é€Ÿç‡é™åˆ¶é”™è¯¯: {str(e)}"
            logger.warning(error_msg)
            return _create_error_result("RateLimitError", error_msg)
        except APIConnectionError as e:
            error_msg = f"ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return _create_error_result("APIConnectionError", error_msg)
        except APIError as e:
            error_msg = f"API é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return _create_error_result("APIError", error_msg)
        except Timeout as e:
            error_msg = f"è¯·æ±‚è¶…æ—¶: {str(e)}"
            logger.error(error_msg)
            return _create_error_result("Timeout", error_msg)
        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return _create_error_result("UnknownError", error_msg)

    return wrapper


def _create_error_result(error_type: str, error_message: str) -> Dict[str, Any]:
    """åˆ›å»ºé”™è¯¯ç»“æœçš„è¾…åŠ©å‡½æ•°"""
    return {
        "success": False,
        "error_type": error_type,
        "error_message": error_message,
        "data": None
    }


def _get_usage_info(usage) -> Dict[str, Any]:
    """è·å–ä½¿ç”¨æƒ…å†µä¿¡æ¯ï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬"""
    if hasattr(usage, 'model_dump'):
        return usage.model_dump()
    elif hasattr(usage, 'dict'):
        return usage.dict()
    else:
        return {}


def _log_api_call_start(model: str, messages_count: int):
    """è®°å½• API è°ƒç”¨å¼€å§‹"""
    logger.info(f"å¼€å§‹è°ƒç”¨ LLM API: model={model}, messages_count={messages_count}")


def _log_api_call_success(model: str, tokens_used: int):
    """è®°å½• API è°ƒç”¨æˆåŠŸ"""
    logger.info(f"LLM API è°ƒç”¨æˆåŠŸ: model={model}, tokens_used={tokens_used}")


class LLMClient:
    def __init__(self, api_key: str = None, base_url: str = DEFAULT_BASE_URL):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            api_key: API å¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è·å–
            base_url: API åŸºç¡€ URL
        """
        if api_key is None:
            api_key = os.getenv('DASHSCOPE_API_KEY')
            if not api_key:
                raise ValueError("è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")

        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )

    @handle_llm_exceptions
    def call_llm(
            self,
            messages: List[Dict],
            model: str = DEFAULT_MODEL,
            temperature: float = 0.7,
            max_tokens: int = 2000,
            response_format: Optional[Dict] = None,
            retry_times: int = 3,
            retry_delay: float = 1.0
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ LLM API å¹¶å¤„ç†å¼‚å¸¸ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            response_format: å“åº”æ ¼å¼
            retry_times: é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

        Returns:
            åŒ…å«å“åº”ç»“æœæˆ–é”™è¯¯ä¿¡æ¯çš„å­—å…¸
        """
        import time

        for attempt in range(retry_times):
            try:
                # è®°å½•è°ƒç”¨å¼€å§‹
                _log_api_call_start(model, len(messages))

                # æ„å»ºè¯·æ±‚å‚æ•°
                params = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "extra_body": {"enable_thinking": False}
                }

                if response_format:
                    params["response_format"] = response_format

                # æ‰§è¡Œ API è°ƒç”¨
                completion = self.client.chat.completions.create(**params)

                # è§£æå“åº”
                usage_info = _get_usage_info(completion.usage)
                result = {
                    "success": True,
                    "data": completion.choices[0].message.content,
                    "model": completion.model,
                    "usage": usage_info,
                    "request_id": getattr(completion, 'id', None)
                }

                tokens_used = result['usage'].get('total_tokens', 0)
                _log_api_call_success(model, tokens_used)

                return result

            except (RateLimitError, APIConnectionError, Timeout) as e:
                # å¯¹äºå¯é‡è¯•çš„é”™è¯¯è¿›è¡Œé‡è¯•
                if attempt < retry_times - 1:
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ{retry_delay} ç§’åé‡è¯•: {str(e)}")
                    time.sleep(retry_delay)
                    continue
                else:
                    # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼ŒæŠ›å‡ºå¼‚å¸¸è®©è£…é¥°å™¨å¤„ç†
                    raise e
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸ç›´æ¥æŠ›å‡º
                raise e


# ä¾¿æ·è°ƒç”¨å‡½æ•°
def llm_call(
        prompt: str,
        system_prompt: str = "You are a helpful assistant.",
        model: str = DEFAULT_MODEL,
        retry_times: int = 3,
        **kwargs
) -> Dict[str, Any]:
    """
    ä¾¿æ·çš„ LLM è°ƒç”¨å‡½æ•°

    Args:
        prompt: ç”¨æˆ·è¾“å…¥æç¤º
        system_prompt: ç³»ç»Ÿæç¤º
        model: æ¨¡å‹åç§°
        retry_times: é‡è¯•æ¬¡æ•°
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        API è°ƒç”¨ç»“æœ
    """
    # ä»ç¯å¢ƒå˜é‡è·å– API å¯†é’¥
    api_key = os.getenv('DASHSCOPE_API_KEY')
    if not api_key:
        logger.error("æœªæ‰¾åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return _create_error_result("ConfigurationError", "æœªæ‰¾åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")

    client = LLMClient(api_key=api_key)

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]

    return client.call_llm(messages, model=model, retry_times=retry_times, **kwargs)


# é‡æ„çš„ StructuredAgent ç±»
class StructuredAgent:
    def __init__(self, api_key: str = None):
        self.llm_client = LLMClient(api_key)

    def create_math_prompt(self, problem: str) -> str:
        """åˆ›å»ºæ•°å­¦è§£é¢˜çš„ç»“æ„åŒ–æç¤ºè¯"""
        return f"""
è¯·ä½¿ç”¨æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰æ–¹æ³•è§£å†³ä»¥ä¸‹æ•°å­¦é—®é¢˜ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æ„åŒ–ç»“æœï¼š

é—®é¢˜ï¼š{problem}

è¦æ±‚æ­¥éª¤ï¼š
1. é¦–å…ˆåˆ†æé—®é¢˜çš„å…³é”®ä¿¡æ¯
2. åˆ—å‡ºè§£é¢˜æ€è·¯å’Œæ­¥éª¤
3. é€æ­¥æ¨å¯¼è®¡ç®—è¿‡ç¨‹
4. å¾—å‡ºæœ€ç»ˆç­”æ¡ˆ
5. éªŒè¯ç­”æ¡ˆçš„åˆç†æ€§

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
  "problem": "...",
  "analysis": "...",
  "steps": [
    {{
      "step_number": 1,
      "description": "...",
      "calculation": "..."
    }}
  ],
  "final_answer": "...",
  "verification": "..."
}}
"""

    def create_copywriting_prompt(self, requirements: str) -> str:
        """åˆ›å»ºæ–‡æ¡ˆç”Ÿæˆçš„ç»“æ„åŒ–æç¤ºè¯"""
        return f"""
è¯·ä½¿ç”¨æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰æ–¹æ³•ç”Ÿæˆæ»¡è¶³ä»¥ä¸‹è¦æ±‚çš„æ–‡æ¡ˆï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æ„åŒ–ç»“æœï¼š

éœ€æ±‚ï¼š{requirements}

è¦æ±‚æ­¥éª¤ï¼š
1. åˆ†ææ–‡æ¡ˆç›®æ ‡å—ä¼—å’Œç›®çš„
2. ç¡®å®šæ–‡æ¡ˆé£æ ¼å’Œè¯­è°ƒ
3. æ„æ€æ ¸å¿ƒä¿¡æ¯å’Œè¦ç‚¹
4. ç»„ç»‡æ–‡æ¡ˆç»“æ„
5. æ’°å†™æ–‡æ¡ˆå†…å®¹

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
  "requirement": "...",
  "target_audience": "...",
  "style_tone": "...",
  "key_points": ["...", "..."],
  "structure_plan": "...",
  "generated_copy": "...",
  "call_to_action": "..."
}}
"""

    def chat_completion(self, prompt: str, model: str = DEFAULT_MODEL, retry_times: int = 3) -> Dict[str, Any]:
        """é€šç”¨èŠå¤©å®Œæˆæ–¹æ³•"""
        result = self.llm_client.call_llm(
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿ä½¿ç”¨æ€ç»´é“¾æ–¹æ³•è¿›è¡Œé€»è¾‘æ¨ç†å’Œå†…å®¹åˆ›ä½œã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æ„åŒ–è¾“å‡ºã€‚"
                },
                {"role": "user", "content": prompt}
            ],
            model=model,
            response_format={"type": "json_object"},
            retry_times=retry_times
        )

        if result["success"]:
            try:
                parsed_content = json.loads(result["data"])
                result["parsed_data"] = parsed_content
            except json.JSONDecodeError:
                logger.error("JSON è§£æå¤±è´¥")
                result["parsed_data"] = None

        return result


class ConversationAgent:
    def __init__(self, api_key: str = None):
        self.agent = StructuredAgent(api_key=api_key)
        # å­˜å‚¨å¯¹è¯å†å²
        self.conversation_history: List[Dict[str, str]] = []
        # ç”¨æˆ·è¾“å…¥éªŒè¯è§„åˆ™
        self.input_validator = InputValidator()

    def validate_input(self, user_input: str) -> tuple[bool, str]:
        """éªŒè¯ç”¨æˆ·è¾“å…¥çš„åˆæ³•æ€§"""
        return self.input_validator.validate(user_input)

    def add_to_history(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        self.conversation_history.append({
            "role": role,
            "content": content,
            "timestamp": self._get_timestamp()
        })

    def get_conversation_context(self) -> List[Dict[str, str]]:
        """è·å–å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡"""
        return self.conversation_history.copy()

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history.clear()

    def chat(self, user_input: str) -> Dict[str, Any]:
        # éªŒè¯è¾“å…¥
        is_valid, validation_msg = self.validate_input(user_input)
        if not is_valid:
            return {
                "success": False,
                "error_type": "InputValidationError",
                "error_message": validation_msg,
                "data": None
            }

        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²è®°å½•
        self.add_to_history("user", user_input)

        try:
            # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å²
            messages = self._build_messages()

            # è°ƒç”¨ LLM
            response = self.agent.chat_completion(
                prompt=self._format_current_prompt(user_input),
                retry_times=2
            )

            if response["success"]:
                # å®‰å…¨åœ°è·å–åŠ©æ‰‹å›å¤
                parsed_data = response.get("parsed_data")
                if isinstance(parsed_data, dict):
                    assistant_reply = parsed_data.get("generated_copy", response["data"])
                elif isinstance(parsed_data, (int, float)):  # å¦‚æœæ˜¯æ•°å­—ç±»å‹
                    assistant_reply = str(parsed_data)
                else:
                    assistant_reply = response.get("data", "")

                self.add_to_history("assistant", assistant_reply)
                return response
            else:
                # å‘ç”Ÿé”™è¯¯æ—¶ä»è®°å½•åˆ°å†å²
                error_msg = f"åŠ©æ‰‹æš‚æ—¶æ— æ³•å›åº”: {response['error_message']}"
                self.add_to_history("assistant", error_msg)
                return response  # ç¡®ä¿è¿”å›response

        except Exception as e:
            error_msg = f"å¯¹è¯å¤„ç†å‡ºé”™: {str(e)}"
            self.add_to_history("assistant", error_msg)
            logger.error(error_msg)
            return {
                "success": False,
                "error_type": "ConversationError",
                "error_message": error_msg,
                "data": None
            }

    def _build_messages(self) -> List[Dict[str, str]]:
        """æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨"""
        # ä½¿ç”¨æœ€è¿‘çš„å‡ è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
        recent_history = self.conversation_history[-6:]  # æœ€è¿‘3è½®å¯¹è¯ï¼ˆç”¨æˆ·+åŠ©æ‰‹ï¼‰

        messages = [{
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤šè½®å¯¹è¯ã€‚è¯·å‚è€ƒä¹‹å‰çš„å¯¹è¯å†å²æ¥å›ç­”é—®é¢˜ã€‚"
        }]

        for msg in recent_history:
            messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })

        return messages

    def _format_current_prompt(self, user_input: str) -> str:
        """æ ¼å¼åŒ–å½“å‰ç”¨æˆ·çš„è¾“å…¥"""
        if len(self.conversation_history) <= 2:  # åªæœ‰å½“å‰è¿™æ¬¡è¾“å…¥
            return user_input
        else:
            # æä¾›å¯¹è¯ä¸Šä¸‹æ–‡
            context = "\n".join([
                f"{msg['role']}: {msg['content']}"
                for msg in self.conversation_history[-4:]  # æœ€è¿‘2è½®
            ])
            return f"ä¹‹å‰çš„å¯¹è¯:\n{context}\n\nç°åœ¨çš„é—®é¢˜: {user_input}"

    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


class InputValidator:
    """ç”¨æˆ·è¾“å…¥éªŒè¯å™¨"""

    def __init__(self):
        # å®šä¹‰éæ³•å­—ç¬¦æ¨¡å¼
        self.invalid_patterns = [
            r'[<>{}[\]\\]',  # HTML/XMLæ ‡ç­¾å­—ç¬¦
            r'(\n\s*){3,}',  # è¿‡å¤šçš„ç©ºè¡Œ
        ]
        # å®šä¹‰æœ€å¤§è¾“å…¥é•¿åº¦
        self.max_length = 1000
        # å®šä¹‰æœ€å°æœ‰æ•ˆé•¿åº¦
        self.min_length = 1

    def validate(self, user_input: str) -> tuple[bool, str]:
        """éªŒè¯ç”¨æˆ·è¾“å…¥"""
        if user_input is None:
            return False, "è¾“å…¥ä¸èƒ½ä¸ºç©º"

        # å»é™¤é¦–å°¾ç©ºç™½
        cleaned_input = user_input.strip()

        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not cleaned_input:
            return False, "è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹"

        # æ£€æŸ¥é•¿åº¦
        if len(cleaned_input) < self.min_length:
            return False, "è¾“å…¥å†…å®¹å¤ªçŸ­ï¼Œè¯·è¾“å…¥æ›´å¤šå†…å®¹"

        if len(cleaned_input) > self.max_length:
            return False, f"è¾“å…¥å†…å®¹å¤ªé•¿ï¼Œæœ€å¤šå…è®¸{self.max_length}ä¸ªå­—ç¬¦"

        # æ£€æŸ¥éæ³•å­—ç¬¦
        for pattern in self.invalid_patterns:
            if re.search(pattern, cleaned_input):
                return False, "è¾“å…¥åŒ…å«éæ³•å­—ç¬¦ï¼Œè¯·é‡æ–°è¾“å…¥"

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šç‰¹æ®Šå­—ç¬¦
        special_char_ratio = sum(1 for c in cleaned_input if not c.isalnum() and not c.isspace()) / len(cleaned_input)
        if special_char_ratio > 0.7:  # ç‰¹æ®Šå­—ç¬¦å æ¯”è¶…è¿‡70%
            return False, "è¾“å…¥åŒ…å«è¿‡å¤šç‰¹æ®Šå­—ç¬¦ï¼Œè¯·ä½¿ç”¨æ­£å¸¸æ–‡å­—"

        return True, "è¾“å…¥æœ‰æ•ˆ"


def interactive_chat():
    """äº¤äº’å¼èŠå¤©ç•Œé¢"""
    print("=== AI å¯¹è¯åŠ©æ‰‹ ===")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºå¯¹è¯")
    print("è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("è¾“å…¥ 'history' æŸ¥çœ‹å¯¹è¯å†å²")
    print("-" * 30)

    # åˆå§‹åŒ–å¯¹è¯åŠ©æ‰‹
    agent = ConversationAgent()

    while True:
        try:
            user_input = input("\næ‚¨: ").strip()

            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("å†è§ï¼")
                break
            elif user_input.lower() == 'clear':
                agent.clear_history()
                print("ğŸ¤– åŠ©æ‰‹: å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            elif user_input.lower() == 'history':
                history = agent.get_conversation_context()
                if history:
                    print("ğŸ¤– å¯¹è¯å†å²:")
                    for i, msg in enumerate(history, 1):
                        print(f"  {i}. [{msg['role']}] {msg['content']}")
                else:
                    print("ğŸ¤– åŠ©æ‰‹: å½“å‰æ²¡æœ‰å¯¹è¯å†å²")
                continue
            elif not user_input:
                print("ğŸ¤– åŠ©æ‰‹: è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹")
                continue

            # å¤„ç†ç”¨æˆ·è¾“å…¥
            result = agent.chat(user_input)

            if result["success"]:
                response_data = result.get("parsed_data") or result.get("data")
                if isinstance(response_data, dict):
                    # å¦‚æœæ˜¯ç»“æ„åŒ–æ•°æ®ï¼Œæå–ä¸»è¦å†…å®¹
                    content = response_data.get("generated_copy") or response_data.get("final_answer") or str(
                        response_data)
                else:
                    content = response_data

                print(f"ğŸ¤– åŠ©æ‰‹: {content}")
            else:
                print(f"ğŸ¤– åŠ©æ‰‹: {result['error_message']}")

        except KeyboardInterrupt:
            print("\n\nå¯¹è¯è¢«ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"ğŸ¤– åŠ©æ‰‹: å‘ç”Ÿé”™è¯¯ - {str(e)}")


if __name__ == '__main__':
    # è¿è¡Œäº¤äº’å¼èŠå¤©
    interactive_chat()
